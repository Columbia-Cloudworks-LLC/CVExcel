# CVExcel Cursor Rules
# Cursor-Native AI Development

## Project Context
This is a CVE (Common Vulnerabilities and Exposures) data scraping project that:
- Scrapes vendor advisory pages for vulnerability information
- Uses PowerShell for automation and data processing
- Implements Playwright for JavaScript-heavy web scraping
- Follows NIST security guidelines for secure coding practices
- Uses modular vendor-specific scraping classes

## Development Philosophy
- **Cursor-First Development** - Use Cursor's native AI chat for all code changes
- **No PowerShell Scripting for AI** - AI-driven development should happen in Cursor, not via PowerShell scripts
- **Direct Code Changes** - All development happens through Cursor chat interface
- **Idempotent Changes** - Cursor handles change application automatically

## Core Principles
1. **Security First** - All code must follow NIST security guidelines
2. **PowerShell Best Practices** - Follow Microsoft's recommended patterns
3. **Playwright Excellence** - Use modern testing and automation practices
4. **Maintainability** - Write clean, documented, and modular code
5. **Error Handling** - Implement comprehensive error handling and logging
6. **Cursor Native AI** - Use Cursor's built-in AI capabilities for all development

## Project Structure
- `vendors/` - Modular vendor-specific scraping classes
- `tests/` - Automated testing suite using Pester and Playwright
- `out/` - Output directory for scraped data (CSV format)
- `docs/` - Project documentation and guides
- `config/` - Configuration files and settings

## Key Technologies
- **PowerShell** - Primary scripting language with advanced function patterns
- **Playwright** - Web automation and testing framework
- **PSScriptAnalyzer** - Code quality and security analysis
- **Pester** - PowerShell testing framework
- **Cursor AI** - Native AI-driven development and code assistance

## Security Requirements
- Follow NIST SP 800-53 security controls
- Implement proper input validation and sanitization
- Use secure coding practices for all web scraping operations
- Maintain audit trails and comprehensive logging
- Handle sensitive data according to security guidelines

## Development Workflow
1. **Cursor Chat** - Request changes directly through Cursor chat
2. **Direct Implementation** - Cursor implements changes in real-time
3. **Automated Testing** - Run tests manually or through Cursor
4. **Git Integration** - Commit changes through Cursor or git CLI
5. **Documentation** - Maintain comprehensive documentation

## Code Quality Standards
- Use PSScriptAnalyzer for code analysis
- Follow PowerShell best practices and style guidelines
- Implement comprehensive error handling
- Write clear, documented code with proper comments
- Use modular design patterns for maintainability

## Testing Requirements
- Write tests for all new functionality
- Use Pester for PowerShell unit testing
- Use Playwright for web automation testing
- Maintain test coverage for critical paths
- Test vendor-specific scraping modules

## Cursor Chat Development
When making changes through Cursor chat:
1. Describe the exact change needed in natural language
2. Cursor will analyze and implement the change
3. Review the changes carefully before accepting
4. Run tests to verify functionality
5. Commit changes when ready

## File Patterns
- PowerShell scripts: `*.ps1`, `*.psm1`
- Configuration: `*.yaml`, `*.json`
- Documentation: `*.md`
- Test files: `test-*.ps1`, `TEST_*.ps1`
- Vendor modules: `vendors/*Vendor.ps1`

## Common Tasks
- Adding new vendor modules
- Improving scraping accuracy
- Enhancing error handling
- Updating documentation
- Optimizing performance
- Security improvements

## Error Handling
- Always use try-catch blocks
- Log errors with context
- Provide meaningful error messages
- Implement retry logic for network operations
- Validate inputs before processing

## Performance Considerations
- Use parallel processing where appropriate
- Implement rate limiting for API calls
- Cache frequently accessed data
- Optimize web scraping operations
- Monitor memory usage for large datasets
